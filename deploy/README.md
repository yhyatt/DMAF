# DMAF Cloud Deployment Guide

This guide walks you through deploying DMAF to Google Cloud Platform using Cloud Run Jobs for cost-optimized batch processing.

**Target Cost**: $0-5/month within GCP free tier

---

## Architecture Overview

```
┌─────────────────┐      ┌──────────────────┐      ┌─────────────────┐
│ Cloud Scheduler │ ───> │ Cloud Run Job    │ ───> │ Google Photos   │
│ (every 3 hours) │      │ (scan-once mode) │      │ API             │
└─────────────────┘      └──────────────────┘      └─────────────────┘
                                  │
                                  ├─> GCS Bucket (WhatsApp media)
                                  │
                                  └─> Firestore (deduplication DB)
```

**Key Features**:
- **Scale-to-zero**: No cost when not processing
- **Serverless**: No server management
- **Automated**: Runs on schedule without manual intervention
- **Free tier eligible**: All services have generous free tiers

---

## Prerequisites

### 1. Google Cloud Platform Setup

```bash
# Install gcloud CLI (if not already installed)
# See: https://cloud.google.com/sdk/docs/install

# Login to GCP
gcloud auth login

# Create a new project (or use existing)
export PROJECT_ID="dmaf-production"  # Change to your project ID
gcloud projects create $PROJECT_ID --name="DMAF Production"

# Set current project
gcloud config set project $PROJECT_ID

# Enable billing (required for Cloud Run, even with free tier)
# Do this via console: https://console.cloud.google.com/billing

# Enable required APIs
gcloud services enable \
  run.googleapis.com \
  cloudbuild.googleapis.com \
  cloudscheduler.googleapis.com \
  firestore.googleapis.com \
  secretmanager.googleapis.com \
  storage.googleapis.com
```

### 2. Local Prerequisites

- Docker installed locally (for testing)
- OAuth credentials (`client_secret.json`) from Google Cloud Console
- OAuth token (`token.json`) - generated by running locally first
- Known people images in `data/known_people/`

---

## Step 1: Configure Firestore

Firestore stores the deduplication database (which files have been processed).

```bash
# Create Firestore database (choose region close to you)
gcloud firestore databases create --location=nam5

# Verify creation
gcloud firestore databases list
```

**Expected output**:
```
NAME                CREATE_TIME
(default)           2024-01-15T10:30:00
```

---

## Step 2: Store Secrets

Store OAuth credentials and tokens in Secret Manager:

```bash
# Create secret for OAuth client credentials
gcloud secrets create dmaf-oauth-client \
  --data-file=client_secret.json

# Create secret for OAuth token (run app locally first to generate token.json)
gcloud secrets create dmaf-oauth-token \
  --data-file=token.json

# Create secret for config file
gcloud secrets create dmaf-config \
  --data-file=config.yaml

# Verify secrets
gcloud secrets list
```

---

## Step 3: Create GCS Bucket for WhatsApp Media

This bucket will store WhatsApp media synced from your phone.

```bash
# Create bucket (choose region matching Firestore)
gsutil mb -l us-central1 gs://$PROJECT_ID-whatsapp-media

# Set lifecycle rule to auto-delete after processing (optional)
cat > lifecycle.json << EOF
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "Delete"},
        "condition": {
          "age": 30,
          "matchesPrefix": ["processed/"]
        }
      }
    ]
  }
}
EOF

gsutil lifecycle set lifecycle.json gs://$PROJECT_ID-whatsapp-media
```

---

## Step 4: Build and Push Docker Image

### Option A: Using Cloud Build (Recommended)

```bash
# Submit build to Cloud Build (uses cloudbuild.yaml)
gcloud builds submit --config cloudbuild.yaml

# Verify image
gcloud container images list --repository=gcr.io/$PROJECT_ID
```

### Option B: Build Locally and Push

```bash
# Build image locally
docker build -t gcr.io/$PROJECT_ID/dmaf:latest .

# Authenticate Docker with GCR
gcloud auth configure-docker

# Push image
docker push gcr.io/$PROJECT_ID/dmaf:latest
```

**Expected build time**: 3-5 minutes (includes InsightFace model download ~600MB)

---

## Step 5: Create Cloud Run Job

Cloud Run Jobs execute the batch processing on a schedule.

```bash
# Create service account for the job
gcloud iam service-accounts create dmaf-runner \
  --display-name="DMAF Cloud Run Job Service Account"

export SA_EMAIL="dmaf-runner@$PROJECT_ID.iam.gserviceaccount.com"

# Grant necessary permissions
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$SA_EMAIL" \
  --role="roles/datastore.user"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$SA_EMAIL" \
  --role="roles/storage.objectViewer"

gcloud secrets add-iam-policy-binding dmaf-oauth-client \
  --member="serviceAccount:$SA_EMAIL" \
  --role="roles/secretmanager.secretAccessor"

gcloud secrets add-iam-policy-binding dmaf-oauth-token \
  --member="serviceAccount:$SA_EMAIL" \
  --role="roles/secretmanager.secretAccessor"

gcloud secrets add-iam-policy-binding dmaf-config \
  --member="serviceAccount:$SA_EMAIL" \
  --role="roles/secretmanager.secretAccessor"

# Create Cloud Run Job
gcloud run jobs create dmaf-scan \
  --image=gcr.io/$PROJECT_ID/dmaf:latest \
  --region=us-central1 \
  --service-account=$SA_EMAIL \
  --memory=1Gi \
  --cpu=1 \
  --max-retries=1 \
  --task-timeout=15m \
  --set-secrets="/secrets/client_secret.json=dmaf-oauth-client:latest" \
  --set-secrets="/secrets/token.json=dmaf-oauth-token:latest" \
  --set-secrets="/config/config.yaml=dmaf-config:latest" \
  --args="--scan-once,--config,/config/config.yaml"
```

**Parameters explained**:
- `--memory=1Gi`: 1GB RAM (sufficient for face recognition)
- `--cpu=1`: 1 vCPU (adequate for batch processing)
- `--max-retries=1`: Retry once if job fails
- `--task-timeout=15m`: Maximum runtime (adjust based on media volume)

---

## Step 6: Test Manual Execution

Before scheduling, test the job manually:

```bash
# Execute job manually
gcloud run jobs execute dmaf-scan --region=us-central1

# Follow execution in real-time
gcloud logging read "resource.type=cloud_run_job" \
  --limit=50 \
  --format="table(timestamp, textPayload)" \
  --freshness=5m
```

**Expected log output**:
```
2024-01-15 10:30:00  INFO - Using face recognition backend: insightface
2024-01-15 10:30:01  INFO - Using Firestore: project=dmaf-production, collection=dmaf_files
2024-01-15 10:30:02  INFO - Running in batch mode (scan-once)
2024-01-15 10:30:05  INFO - Batch scan complete: 5 new, 5 processed, 3 matched, 3 uploaded, 0 errors
```

---

## Step 7: Set Up Cloud Scheduler

Automate execution with Cloud Scheduler:

```bash
# Create scheduler job (runs every 3 hours)
gcloud scheduler jobs create http dmaf-schedule \
  --location=us-central1 \
  --schedule="0 */3 * * *" \
  --uri="https://us-central1-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/$PROJECT_ID/jobs/dmaf-scan:run" \
  --http-method=POST \
  --oauth-service-account-email=$SA_EMAIL

# Test scheduler immediately
gcloud scheduler jobs run dmaf-schedule --location=us-central1

# Check scheduler status
gcloud scheduler jobs describe dmaf-schedule --location=us-central1
```

**Schedule options**:
- `0 */3 * * *` - Every 3 hours (recommended - stays in free tier)
- `*/15 * * * *` - Every 15 minutes (faster, ~$1-3/month overage)
- `0 * * * *` - Every hour
- `0 0 * * *` - Once daily at midnight

---

## Step 8: Sync WhatsApp Media to GCS

You need a method to sync WhatsApp media from your phone to the GCS bucket.

### Option A: FolderSync Pro (Android) - Easiest

1. Install [FolderSync Pro](https://play.google.com/store/apps/details?id=dk.tacit.android.foldersync.full) ($5 one-time)
2. Create GCP service account key:
   ```bash
   gcloud iam service-accounts keys create sync-key.json \
     --iam-account=$SA_EMAIL
   ```
3. Configure in FolderSync:
   - Account type: Google Cloud Storage
   - Upload service account key
   - Bucket: `$PROJECT_ID-whatsapp-media`
   - Local folder: `/storage/emulated/0/WhatsApp/Media/WhatsApp Images/`
   - Sync type: To remote only
   - Schedule: Every 3 hours (match Cloud Scheduler frequency)

### Option B: rclone + Termux (Free, Technical)

1. Install [Termux](https://f-droid.org/en/packages/com.termux/) from F-Droid
2. Set up rclone:
   ```bash
   # In Termux
   pkg install rclone
   termux-setup-storage

   # Configure rclone for GCS (interactive)
   rclone config

   # Sync script
   cat > ~/sync-whatsapp.sh << 'EOF'
   #!/data/data/com.termux/files/usr/bin/bash
   rclone sync ~/storage/dcim/WhatsApp/WhatsApp\ Images/ \
     gcs:$PROJECT_ID-whatsapp-media/ \
     --log-file ~/rclone.log
   EOF

   chmod +x ~/sync-whatsapp.sh

   # Add to crontab
   crontab -e
   # Add: 0 */3 * * * ~/sync-whatsapp.sh
   ```

### Option C: Syncthing (Free, Cross-Platform)

1. Install [Syncthing](https://syncthing.net/) on phone and a VM/server
2. Sync WhatsApp folder to VM
3. Use `gsutil rsync` on VM to push to GCS:
   ```bash
   gsutil -m rsync -r /path/to/synced/WhatsApp/ \
     gs://$PROJECT_ID-whatsapp-media/
   ```

---

## Step 9: Monitor and Optimize Costs

### Set Up Billing Alerts

```bash
# Get billing account ID
gcloud billing accounts list

export BILLING_ACCOUNT="XXXXXX-YYYYYY-ZZZZZZ"

# Create budget alert
gcloud billing budgets create \
  --billing-account=$BILLING_ACCOUNT \
  --display-name="DMAF Monthly Budget" \
  --budget-amount=10USD \
  --threshold-rule=percent=50 \
  --threshold-rule=percent=90 \
  --threshold-rule=percent=100
```

### Monitor Usage

```bash
# Check Cloud Run job executions
gcloud run jobs executions list \
  --job=dmaf-scan \
  --region=us-central1 \
  --limit=10

# Check Firestore usage
gcloud firestore operations list

# Estimate costs
gcloud billing accounts get-iam-policy $BILLING_ACCOUNT
```

### Expected Costs (within free tier)

| Service | Free Tier | Expected Usage | Cost |
|---------|-----------|----------------|------|
| Cloud Run Jobs | 180K vCPU-sec, 360K GiB-sec/mo | ~240 executions/mo (3-hourly) | $0 |
| Cloud Scheduler | 3 free jobs | 1 job | $0 |
| Firestore | 1GB storage, 50K reads/day | <100MB, ~1K reads/day | $0 |
| Secret Manager | 6 active secrets | 3 secrets | $0 |
| GCS | 5GB storage (US regions) | <1GB | $0 |
| Artifact Registry | 0.5GB free | ~0.7GB image | ~$0.02/mo |
| **Total** | | | **$0-5/mo** |

---

## Troubleshooting

### Job Fails with "Permission Denied"

**Symptom**: Job logs show "403 Forbidden" or "Permission denied"

**Solution**: Verify service account permissions:
```bash
gcloud projects get-iam-policy $PROJECT_ID \
  --flatten="bindings[].members" \
  --filter="bindings.members:$SA_EMAIL"
```

### Job Timeout

**Symptom**: Job exceeds 15 minute timeout

**Solution**: Increase timeout or reduce processing:
```bash
gcloud run jobs update dmaf-scan \
  --region=us-central1 \
  --task-timeout=30m
```

### InsightFace Model Download Fails

**Symptom**: "Unable to download buffalo_l model"

**Solution**: Model is pre-downloaded in Docker image. Verify image:
```bash
docker run gcr.io/$PROJECT_ID/dmaf:latest \
  python -c "from insightface.app import FaceAnalysis; FaceAnalysis(name='buffalo_l')"
```

### Firestore Connection Error

**Symptom**: "Could not connect to Firestore"

**Solution**: Verify Firestore is created and service account has access:
```bash
gcloud firestore databases list
gcloud projects get-iam-policy $PROJECT_ID | grep datastore.user
```

### No Images Being Processed

**Symptom**: Job runs successfully but no images uploaded

**Solution**:
1. Verify GCS bucket has images: `gsutil ls gs://$PROJECT_ID-whatsapp-media/`
2. Check face recognition matches in logs
3. Verify known_people directory is populated in config

---

## Updating Deployment

### Update Code

```bash
# Make code changes
git commit -am "Update feature X"

# Rebuild and push image
gcloud builds submit --config cloudbuild.yaml

# Cloud Run Job automatically uses :latest tag
# No manual update needed if using :latest
```

### Update Configuration

```bash
# Update config.yaml locally

# Update secret
gcloud secrets versions add dmaf-config \
  --data-file=config.yaml

# Verify new version is used (restart may be needed)
gcloud run jobs describe dmaf-scan --region=us-central1
```

### Update Secrets

```bash
# Update OAuth token (if refreshed)
gcloud secrets versions add dmaf-oauth-token \
  --data-file=token.json

# Verify
gcloud secrets versions list dmaf-oauth-token
```

---

## Cleanup / Uninstall

To completely remove the deployment:

```bash
# Delete Cloud Run job
gcloud run jobs delete dmaf-scan --region=us-central1

# Delete Cloud Scheduler job
gcloud scheduler jobs delete dmaf-schedule --location=us-central1

# Delete secrets
gcloud secrets delete dmaf-oauth-client
gcloud secrets delete dmaf-oauth-token
gcloud secrets delete dmaf-config

# Delete GCS bucket
gsutil -m rm -r gs://$PROJECT_ID-whatsapp-media

# Delete Firestore database (careful - this deletes all data!)
gcloud firestore databases delete --database='(default)'

# Delete service account
gcloud iam service-accounts delete $SA_EMAIL

# Delete project (if dedicated project)
gcloud projects delete $PROJECT_ID
```

---

## Alternative: Even Lower Cost Options

If $5/month is still too much:

### Raspberry Pi at Home ($0/month)
- Run DMAF in watcher mode on Raspberry Pi
- One-time cost: ~$50 for Pi 4
- Power consumption: ~$1-2/month

### Oracle Cloud Free Tier
- 2 free AMD VMs (1/8 OCPU, 1GB RAM each)
- Run DMAF in Docker container
- Truly free forever (no credit card expiry)

### Fly.io Free Tier
- 3 shared-CPU VMs (256MB RAM each)
- Run DMAF as scheduled task
- Free tier includes 160GB bandwidth

---

## Support

For issues or questions:
- GitHub Issues: https://github.com/yourusername/wa_automate/issues
- Documentation: https://github.com/yourusername/wa_automate/blob/main/README.md

---

## Security Best Practices

1. **Rotate OAuth tokens regularly**: Generate new tokens every 6 months
2. **Use Secret Manager**: Never commit secrets to git
3. **Restrict service account**: Grant minimum required permissions
4. **Enable VPC Service Controls**: For production deployments
5. **Use private GCS buckets**: Don't make WhatsApp media public
6. **Audit logs**: Enable Cloud Audit Logs for compliance
